{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# GAN model\nGenerative Adversarial Networks are used to generate images that never existed before. They learn about the world (objects, animals and so forth) and create new versions of those images that never existed.\n\nThey have two components:\n\n1. A **Generator** - this creates the images.\n2. A **Discriminator** - this assesses the images and tells the generator if they are similar to what it has been trained on. These are based off real world examples.\n\nWhen training the network, both the generator and discriminator start from scratch and learn together.","metadata":{}},{"cell_type":"markdown","source":"## Libraries we'll need","metadata":{}},{"cell_type":"code","source":"from __future__ import print_function\nimport time\nfrom IPython.display import Audio, display\nfrom pathlib import Path\nimport librosa\nimport librosa.display\n\nfrom sklearn.preprocessing import normalize\n\nfrom PIL import Image\n\nimport math\n########### UPDATING PYTORCH IMPORTS AS I INTERPRET FOR SOUNDS\nimport torch\nimport torchaudio\nfrom torch.utils.data import DataLoader\nfrom torchaudio.transforms import Resample, MelSpectrogram\n\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.optim as optim\nimport torch.utils.data\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nfrom torch.autograd import Variable\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom torch import nn, optim\nimport torch.nn.functional as F\nfrom torchvision import datasets, transforms\nfrom torchvision.utils import save_image\n########### ^^^^^^^^^^^^^^^^^^^^^^\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom tqdm import tqdm_notebook as tqdm\nfrom tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2023-04-19T00:44:14.078546Z","iopub.execute_input":"2023-04-19T00:44:14.078943Z","iopub.status.idle":"2023-04-19T00:44:18.400969Z","shell.execute_reply.started":"2023-04-19T00:44:14.078910Z","shell.execute_reply":"2023-04-19T00:44:18.399444Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport os\nimport random\nimport pandas as pd","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-19T00:44:18.403342Z","iopub.execute_input":"2023-04-19T00:44:18.404358Z","iopub.status.idle":"2023-04-19T00:44:18.410760Z","shell.execute_reply.started":"2023-04-19T00:44:18.404314Z","shell.execute_reply":"2023-04-19T00:44:18.409416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dset_path = '/kaggle/input/english-multispeak-corpus-to-overlay-spectrographs'\nworking_dir = '/kaggle/working'\n# CSVs\nfor i in range(5):\n    csvs = [csv for csv in (os.listdir(dset_path)) if os.path.splitext(csv)[1] == '.csv']\nparent_df = pd.read_csv(f'{dset_path}/{csvs[0]}')\nchild_df =  pd.read_csv(f'{dset_path}/{csvs[1]}')\ndf =  pd.read_csv(f'{dset_path}/{csvs[2]}')","metadata":{"execution":{"iopub.status.busy":"2023-04-19T00:44:18.412217Z","iopub.execute_input":"2023-04-19T00:44:18.412681Z","iopub.status.idle":"2023-04-19T00:44:18.498662Z","shell.execute_reply.started":"2023-04-19T00:44:18.412604Z","shell.execute_reply":"2023-04-19T00:44:18.497365Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"overlay_wavs_path = f'{dset_path}/overlay_data/wav/overlays'\nraws_path = f'{dset_path}/raws'","metadata":{"execution":{"iopub.status.busy":"2023-04-19T00:44:18.501256Z","iopub.execute_input":"2023-04-19T00:44:18.501705Z","iopub.status.idle":"2023-04-19T00:44:18.506621Z","shell.execute_reply.started":"2023-04-19T00:44:18.501664Z","shell.execute_reply":"2023-04-19T00:44:18.505560Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"overlay = os.listdir(overlay_wavs_path)[0]\noverlay","metadata":{"execution":{"iopub.status.busy":"2023-04-19T00:44:18.507712Z","iopub.execute_input":"2023-04-19T00:44:18.508046Z","iopub.status.idle":"2023-04-19T00:44:18.671585Z","shell.execute_reply.started":"2023-04-19T00:44:18.508014Z","shell.execute_reply":"2023-04-19T00:44:18.670341Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"lookup helper function to get a random wav (until we get a working dset)","metadata":{}},{"cell_type":"code","source":"def random_lookup():\n    speaker_dirs = os.listdir(raws_path)\n    rand_idx = random.randint(0, len(speaker_dirs)-1)\n    speaker_dir = f'{raws_path}/{speaker_dirs[rand_idx]}'\n                              \n    parent = f'speaker_{rand_idx}'\n    # Child gets passed into DF as int\n    child = rand_idx + random.randint(1,10)\n    \n    print(f'PARENT PATH: {speaker_dir}\\n\\nPARENT: {parent}\\nCHILD: {child}\\n')\n    return speaker_dir, parent, child","metadata":{"execution":{"iopub.status.busy":"2023-04-19T00:44:18.673152Z","iopub.execute_input":"2023-04-19T00:44:18.673493Z","iopub.status.idle":"2023-04-19T00:44:18.680827Z","shell.execute_reply.started":"2023-04-19T00:44:18.673460Z","shell.execute_reply":"2023-04-19T00:44:18.679332Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trio = random_lookup()","metadata":{"execution":{"iopub.status.busy":"2023-04-19T00:44:18.682223Z","iopub.execute_input":"2023-04-19T00:44:18.682664Z","iopub.status.idle":"2023-04-19T00:44:18.710872Z","shell.execute_reply.started":"2023-04-19T00:44:18.682626Z","shell.execute_reply":"2023-04-19T00:44:18.709592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trio","metadata":{"execution":{"iopub.status.busy":"2023-04-19T00:44:18.712311Z","iopub.execute_input":"2023-04-19T00:44:18.713192Z","iopub.status.idle":"2023-04-19T00:44:18.720285Z","shell.execute_reply.started":"2023-04-19T00:44:18.713144Z","shell.execute_reply":"2023-04-19T00:44:18.718990Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lookup_relations(overlay_path, parent, child):\n    raw_path = '/kaggle/input/english-multispeak-corpus-to-overlay-spectrographs/raws'\n    # Get Names\n    parent_name = parent_df.loc[child, parent]\n    parent_id, pfile_id = parent_name.split('_')\n    \n    child_name = child_df.loc[child, parent]\n    child_id, chfilfe_id = child_name.split('_')\n    \n    # Returns\n    p_path = os.path.join(f\"{raw_path}/{parent_id}_samples\", f\"{parent_name}.wav\")\n    c_path = os.path.join(f\"{raw_path}/{child_id}_samples\", f\"{child_name}.wav\")\n    o_path = f\"{overlay_path}\"\n\n    return p_path, c_path, o_path","metadata":{"execution":{"iopub.status.busy":"2023-04-19T00:44:18.721968Z","iopub.execute_input":"2023-04-19T00:44:18.722333Z","iopub.status.idle":"2023-04-19T00:44:18.732762Z","shell.execute_reply.started":"2023-04-19T00:44:18.722296Z","shell.execute_reply":"2023-04-19T00:44:18.731572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_parent_child(overlay_path):\n    overlay_name = (str(overlay_path).split('/')[-1])[:-4]\n\n    parent = overlay_name.split('_and_')[0]\n    child = int(overlay_name.split('_and_')[1].split('_')[1])\n\n    return parent, child","metadata":{"execution":{"iopub.status.busy":"2023-04-19T00:44:18.736054Z","iopub.execute_input":"2023-04-19T00:44:18.736522Z","iopub.status.idle":"2023-04-19T00:44:18.745806Z","shell.execute_reply.started":"2023-04-19T00:44:18.736479Z","shell.execute_reply":"2023-04-19T00:44:18.744566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def lookup(overlay_path):\n    p, c = get_parent_child(overlay_path)\n    p_path, c_path, o_path = lookup_relations(overlay_path, p, c)\n\n    return p_path, c_path, o_path, int(p.split('_')[1]), c","metadata":{"execution":{"iopub.status.busy":"2023-04-19T00:44:18.747497Z","iopub.execute_input":"2023-04-19T00:44:18.748330Z","iopub.status.idle":"2023-04-19T00:44:18.759529Z","shell.execute_reply.started":"2023-04-19T00:44:18.748277Z","shell.execute_reply":"2023-04-19T00:44:18.758173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we have an easy way of returing parent, child, and overlay filenames","metadata":{}},{"cell_type":"code","source":"d, p, c = random_lookup()\np_path, c_path, o_path = lookup_relations(d, p, c)","metadata":{"execution":{"iopub.status.busy":"2023-04-19T00:44:18.898244Z","iopub.execute_input":"2023-04-19T00:44:18.899156Z","iopub.status.idle":"2023-04-19T00:44:18.917845Z","shell.execute_reply.started":"2023-04-19T00:44:18.899103Z","shell.execute_reply":"2023-04-19T00:44:18.916642Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"o_path, p_path, c_path","metadata":{"execution":{"iopub.status.busy":"2023-04-19T00:44:19.041064Z","iopub.execute_input":"2023-04-19T00:44:19.041989Z","iopub.status.idle":"2023-04-19T00:44:19.050896Z","shell.execute_reply.started":"2023-04-19T00:44:19.041943Z","shell.execute_reply":"2023-04-19T00:44:19.049302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## GAN's processes\nFirst, we will give a random noise signal to the ***Generator***, this will create some sound files, which we will use to train the ***Discriminator***. The ***Discriminator*** will be given some **features** we want it to learn, and it will output probabilities.\nThese probabilities are assessed based on their true values, a loss is then calculated and backpropped.\n\n`I think we can use F0 as one of these features`\n\n\nNext we train the generator. We take the batch of sounds that it created and put them through the discriminator again. We do not include the feature sounds. The generator learns by tricking the discriminator into it outputting false positives.\n\nThe discriminator will provide an output of probabilities. The values are then assessed and compared to what they should have been. The error is calculated and backpropagated through the generator and the weights are updated.\n\n\nOver time, this model will be able to recognize it's mistakes in it's generations, and improves because of this.","metadata":{}},{"cell_type":"code","source":"sounds = os.listdir(overlay_wavs_path)\nprint(f'Got paths of {len(sounds)} sounds.')","metadata":{"execution":{"iopub.status.busy":"2023-04-19T00:44:21.202244Z","iopub.execute_input":"2023-04-19T00:44:21.202836Z","iopub.status.idle":"2023-04-19T00:44:21.216030Z","shell.execute_reply.started":"2023-04-19T00:44:21.202780Z","shell.execute_reply":"2023-04-19T00:44:21.214583Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Preprocessing with pytorch\nNow we have paths to audio files. Here are some pytorch methods we can use","metadata":{}},{"cell_type":"code","source":"sample =f'{overlay_wavs_path}/{sounds[6]}'","metadata":{"execution":{"iopub.status.busy":"2023-04-19T00:44:21.560496Z","iopub.execute_input":"2023-04-19T00:44:21.561209Z","iopub.status.idle":"2023-04-19T00:44:21.566697Z","shell.execute_reply.started":"2023-04-19T00:44:21.561165Z","shell.execute_reply":"2023-04-19T00:44:21.565438Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"metadata = torchaudio.info(sample)\nprint(f'Metadata:\\n-----------\\n{metadata}')\n# Loading Audio file\nwaveform, sample_rate = torchaudio.load(sample)\n# By default, dtype=troch.float32 and rand is normalized within [-1, 1]\nprint(f\"----------------------------------------------\\nWaveform is tensor object of shape: {waveform.shape}\\n\")","metadata":{"execution":{"iopub.status.busy":"2023-04-19T00:44:21.743789Z","iopub.execute_input":"2023-04-19T00:44:21.744230Z","iopub.status.idle":"2023-04-19T00:44:21.779863Z","shell.execute_reply.started":"2023-04-19T00:44:21.744192Z","shell.execute_reply":"2023-04-19T00:44:21.778556Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### We can even play the audio","metadata":{}},{"cell_type":"code","source":"def play_audio(waveform, sample_rate):\n    waveform = waveform.numpy()\n    \n    num_channels, num_frames = waveform.shape\n    # MONO\n    if num_channels == 1:\n        display(Audio(waveform[0], rate = sample_rate))\n    # STEREO\n    elif num_channels == 2:\n        display(Audio(waveform[0], waveform[1], rate = sample_rate))\n    else:\n        raise ValueError(\"!!Waveform with more than 2 channels are not supported!!\")\n\nplay_audio(waveform, sample_rate)","metadata":{"execution":{"iopub.status.busy":"2023-04-19T00:44:23.433054Z","iopub.execute_input":"2023-04-19T00:44:23.433499Z","iopub.status.idle":"2023-04-19T00:44:23.466805Z","shell.execute_reply.started":"2023-04-19T00:44:23.433458Z","shell.execute_reply":"2023-04-19T00:44:23.465884Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Filter function\n### Gets only files +-1 std from the mean","metadata":{}},{"cell_type":"code","source":"def sort_wavs(file_list):\n    effects = [['rate', '8000']]\n    print(len(file_list))\n    len_sum = 0\n    len_list = []\n    len_dict_list = []\n    len_dict = {}\n    \n    for i in tqdm(range(len(file_list))):\n        sample = file_list[i]\n        waveform, sample_rate = torchaudio.sox_effects.apply_effects_file(sample, effects)\n        wf = waveform.numpy().flatten()\n        \n        len_list.append(len(wf))\n        len_dict = {'sample': sample, 'length': len(wf)}\n        len_dict_list.append(len_dict)\n        len_sum += len(wf)\n        \n    avg = len_sum / len(file_list)\n    std_dev = np.std(np.array([len_list]))\n    return len_dict_list, std_dev, avg, len_sum\n   \ndef filter_wavs(len_dict_list, std_dev, avg, len_sum):\n    reject = 0\n    accept = 0\n    filtered_file_list = []\n    max_list = []\n    for line in len_dict_list:\n        if line['length'] < avg - std_dev or line['length'] > avg + std_dev:\n            reject += 1\n            continue\n        else:\n            filtered_file_list.append(line['sample'])\n            max_list.append([line['length']])\n            accept += 1\n    print(f\"{accept} files Accepted\\n{reject} files Rejected\\n\")\n    return filtered_file_list, max(max_list)[0]","metadata":{"execution":{"iopub.status.busy":"2023-04-19T00:44:25.146762Z","iopub.execute_input":"2023-04-19T00:44:25.147164Z","iopub.status.idle":"2023-04-19T00:44:25.160352Z","shell.execute_reply.started":"2023-04-19T00:44:25.147130Z","shell.execute_reply":"2023-04-19T00:44:25.158784Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ld, st, avg, lsum = sort_wavs(list(Path(overlay_wavs_path).rglob(\"*.wav\")))","metadata":{"execution":{"iopub.status.busy":"2023-04-19T00:44:26.427621Z","iopub.execute_input":"2023-04-19T00:44:26.428051Z","iopub.status.idle":"2023-04-19T00:48:46.335980Z","shell.execute_reply.started":"2023-04-19T00:44:26.428012Z","shell.execute_reply":"2023-04-19T00:48:46.334745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"st, avg, lsum","metadata":{"execution":{"iopub.status.busy":"2023-04-19T00:48:46.338102Z","iopub.execute_input":"2023-04-19T00:48:46.338498Z","iopub.status.idle":"2023-04-19T00:48:46.347209Z","shell.execute_reply.started":"2023-04-19T00:48:46.338459Z","shell.execute_reply":"2023-04-19T00:48:46.345786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filtered_overlays, padding_size = filter_wavs(ld, st, avg, lsum)","metadata":{"execution":{"iopub.status.busy":"2023-04-19T01:01:42.521357Z","iopub.execute_input":"2023-04-19T01:01:42.522300Z","iopub.status.idle":"2023-04-19T01:01:42.554785Z","shell.execute_reply.started":"2023-04-19T01:01:42.522255Z","shell.execute_reply":"2023-04-19T01:01:42.553081Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Time series embeddings w/ padding for normalization","metadata":{}},{"cell_type":"code","source":"def padding(matrix, desired_size):\n    padding_amt = desired_size - matrix.shape[0] / 2\n    if padding_amt % 2 != 0:    \n        padding_amt = math.floor(padding_amt)\n        return np.pad(matrix, pad_width=((padding_amt, padding_amt + 1), (0, 0)), mode='constant')\n    padding_amt = int(padding_amt)\n    return np.pad(matrix, pad_width=((padding_amt, padding_amt), (0, 0)), mode='constant')","metadata":{"execution":{"iopub.status.busy":"2023-04-19T01:01:43.282761Z","iopub.execute_input":"2023-04-19T01:01:43.283236Z","iopub.status.idle":"2023-04-19T01:01:43.291272Z","shell.execute_reply.started":"2023-04-19T01:01:43.283195Z","shell.execute_reply":"2023-04-19T01:01:43.289675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def waveform_to_tensor(wf, padding_size, embedding_delay = 1, embedding_dimension = 64 * 64 * 3):\n    wf = wf.numpy().flatten()\n    \n    # Initialize embedding matrix\n    M = np.zeros((len(wf) - (embedding_dimension - 1) * embedding_delay, embedding_dimension))\n\n    # Construct embedding by shifting values\n    for i in range(embedding_dimension):\n        M[:, i] = wf[i * embedding_delay:i * embedding_delay + M.shape[0]]\n    \n    M = padding(M, padding_size)\n    \n    # Reshape embedding matrix into 64x64x3 tensor\n    final_tensor = (M.reshape((64, 64, 3, -1)).transpose((3, 0, 1, 2))).astype(\"float16\")\n\n    return final_tensor","metadata":{"execution":{"iopub.status.busy":"2023-04-19T01:01:44.222508Z","iopub.execute_input":"2023-04-19T01:01:44.223689Z","iopub.status.idle":"2023-04-19T01:01:44.232415Z","shell.execute_reply.started":"2023-04-19T01:01:44.223622Z","shell.execute_reply":"2023-04-19T01:01:44.230787Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tensor_to_vector(tensor, embedding_delay = 1 ):\n    matrix = tensor.transpose((1, 2, 3, 0)).reshape(tensor.shape[0], -1)\n    print(matrix.shape)\n    \n    num_windows, embedding_dimension = matrix.shape\n\n    # Calculate length of original time series\n    wf_length = num_windows + (embedding_dimension - 1) * embedding_delay\n    \n    wf_reconstructed = np.zeros((wf_length, ))\n    \n    for i in range(num_windows):\n            window = matrix[i, :]\n            start_index = i * embedding_delay\n            end_index = start_index + embedding_dimension\n            wf_reconstructed[start_index:end_index] = window\n\n    wf_tens = (torch.tensor(wf_reconstructed).unsqueeze(dim=0))\n    return wf_tens","metadata":{"execution":{"iopub.status.busy":"2023-04-19T01:01:44.405681Z","iopub.execute_input":"2023-04-19T01:01:44.406958Z","iopub.status.idle":"2023-04-19T01:01:44.413903Z","shell.execute_reply.started":"2023-04-19T01:01:44.406899Z","shell.execute_reply":"2023-04-19T01:01:44.412925Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def tensor_generator(path, padding):\n    effects = [['rate', '8000']]\n    par, child, over, p_indx, c_indx = lookup(str(path))\n\n    par_wav, par_sr = torchaudio.sox_effects.apply_effects_file(par, effects)\n    over_wav, over_sr = torchaudio.sox_effects.apply_effects_file(over, effects)\n    \n    target = p_indx\n    \n    par_tensor = waveform_to_tensor(par_wav, padding)\n    over_tensor = waveform_to_tensor(over_wav, padding)\n    return par_tensor, over_tensor, target, \n        \n","metadata":{"execution":{"iopub.status.busy":"2023-04-19T01:01:44.641426Z","iopub.execute_input":"2023-04-19T01:01:44.642704Z","iopub.status.idle":"2023-04-19T01:01:44.649755Z","shell.execute_reply.started":"2023-04-19T01:01:44.642654Z","shell.execute_reply":"2023-04-19T01:01:44.648412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# AudioDataset class ","metadata":{}},{"cell_type":"code","source":"class AudioDataset(torch.utils.data.Dataset):  \n    def __init__(self, files, padding_size, transform=None, return_overlay = False):\n        self.transform = transform\n        self.return_overlay = return_overlay\n        self.sound_files = files\n        self.padding_size = padding_size\n \n    def __len__(self):\n        return len(self.sound_files)\n    \n    def __getitem__(self, idx):\n        audio_path = self.sound_files[idx]\n        p, o, t = tensor_generator(audio_path, self.padding_size)\n        return p, o, t\n        ","metadata":{"execution":{"iopub.status.busy":"2023-04-19T01:01:54.692918Z","iopub.execute_input":"2023-04-19T01:01:54.693306Z","iopub.status.idle":"2023-04-19T01:01:54.701411Z","shell.execute_reply.started":"2023-04-19T01:01:54.693271Z","shell.execute_reply":"2023-04-19T01:01:54.700014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_data = AudioDataset(filtered_overlays, padding_size, return_overlay = True)","metadata":{"execution":{"iopub.status.busy":"2023-04-19T01:01:58.606844Z","iopub.execute_input":"2023-04-19T01:01:58.607569Z","iopub.status.idle":"2023-04-19T01:01:58.613228Z","shell.execute_reply.started":"2023-04-19T01:01:58.607515Z","shell.execute_reply":"2023-04-19T01:01:58.612138Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Verify the pipeline is ðŸ‘Œ","metadata":{}},{"cell_type":"code","source":"p, o, t = train_data[3]\nprint(f\"Speaker-Index: {t}\\nO-Shape: {o.shape}\\n\") ","metadata":{"execution":{"iopub.status.busy":"2023-04-19T00:35:05.108771Z","iopub.execute_input":"2023-04-19T00:35:05.109222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"v = tensor_to_vector(o)\nplay_audio(v, 8000)","metadata":{"execution":{"iopub.status.busy":"2023-04-19T00:39:15.038624Z","iopub.execute_input":"2023-04-19T00:39:15.039328Z","iopub.status.idle":"2023-04-19T00:39:15.130872Z","shell.execute_reply.started":"2023-04-19T00:39:15.039288Z","shell.execute_reply":"2023-04-19T00:39:15.129046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now we'll load the dataset and create a dataloader","metadata":{}},{"cell_type":"code","source":"batch_size = 5\ntrain_loader = DataLoader(train_data, shuffle = True, batch_size = batch_size)","metadata":{"execution":{"iopub.status.busy":"2023-04-19T01:02:09.298593Z","iopub.execute_input":"2023-04-19T01:02:09.299147Z","iopub.status.idle":"2023-04-19T01:02:09.305577Z","shell.execute_reply.started":"2023-04-19T01:02:09.299094Z","shell.execute_reply":"2023-04-19T01:02:09.304059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"p, t, o = next(iter(train_loader))","metadata":{"execution":{"iopub.status.busy":"2023-04-19T01:02:10.119882Z","iopub.execute_input":"2023-04-19T01:02:10.120478Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"v = tensor_to_vector(p)\nplay_audio(v, 8000)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Weights\nThe below function simply initializes weights with given distribution mean and SD based on the type of model (Convolutional, or Batch Normalization)","metadata":{}},{"cell_type":"code","source":"def weights_init(model):\n    classname = model.__class__.__name__\n    if classname.find('Conv') != -1:\n        model.weight.data.normal_(0.0, 0.02)\n    elif classname.find('BatchNorm') != -1:\n        model.weight.data.normal_(1.0, 0.02)\n        model.bias.data.fill_(0)\n    else:\n        pass","metadata":{"execution":{"iopub.status.busy":"2023-04-18T19:28:41.882763Z","iopub.status.idle":"2023-04-18T19:28:41.883169Z","shell.execute_reply.started":"2023-04-18T19:28:41.882963Z","shell.execute_reply":"2023-04-18T19:28:41.882984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Generator\nI need to do more research on audio convnets b4 i can continue","metadata":{}},{"cell_type":"code","source":"# Defining the discriminator\nclass D(nn.Module):\n    def __init__(self):\n        super(D, self).__init__()\n        self.main = nn.Sequential(\n                nn.Conv2d(3, 64, 4, stride=2, padding=1, bias=False),\n                nn.LeakyReLU(negative_slope=0.2, inplace=True),\n                nn.Conv2d(64, 128, 4, stride=2, padding=1, bias=False),\n                nn.BatchNorm2d(128),\n                nn.LeakyReLU(negative_slope=0.2, inplace=True),\n                nn.Conv2d(128, 256, 4, stride=2, padding=1, bias=False),\n                nn.BatchNorm2d(256),\n                nn.LeakyReLU(negative_slope=0.2, inplace=True),\n                nn.Conv2d(256, 512, 4, stride=2, padding=1, bias=False),\n                nn.BatchNorm2d(512),\n                nn.LeakyReLU(negative_slope=0.2, inplace=True),\n                nn.Conv2d(512, 1, 4, stride=1, padding=0, bias=False),\n                nn.Sigmoid()\n                )\n        \n    def forward(self, input):\n        output = self.main(input)\n        # .view(-1) = Flattens the output into 1D instead of 2D\n        return output.view(-1)\n        \n    \n# Creating the discriminator\nnetD = D()\nnetD.apply(weights_init)","metadata":{"execution":{"iopub.status.busy":"2023-04-18T19:28:41.885840Z","iopub.status.idle":"2023-04-18T19:28:41.886243Z","shell.execute_reply.started":"2023-04-18T19:28:41.886031Z","shell.execute_reply":"2023-04-18T19:28:41.886052Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class G(nn.Module):\n    def __init__(self):\n        super(G, self).__init__()\n\n        self.main = nn.Sequential(\n            nn.ConvTranspose2d(100, 512, 4, stride=1, padding=0, bias=False),\n            nn.BatchNorm2d(512),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(512, 256, 4, stride=2, padding=(1, 2), bias=False),\n            nn.BatchNorm2d(256),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(128),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1, bias=False),\n            nn.BatchNorm2d(64),\n            nn.ReLU(True),\n            nn.ConvTranspose2d(64, 3, 4, stride=2, padding=1, bias=False),\n            nn.Tanh()\n        )\n        \n        self.adaptive_pool = nn.AdaptiveAvgPool2d((128, 5000))\n\n    def forward(self, input):\n        output = self.main(input)\n        return output\n\n# Creating the generator\nnetG = G()\nnetG.apply(weights_init)\n","metadata":{"execution":{"iopub.status.busy":"2023-04-18T19:28:41.884338Z","iopub.status.idle":"2023-04-18T19:28:41.884766Z","shell.execute_reply.started":"2023-04-18T19:28:41.884531Z","shell.execute_reply":"2023-04-18T19:28:41.884574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir results\n!ls","metadata":{"execution":{"iopub.status.busy":"2023-04-18T19:28:41.888460Z","iopub.status.idle":"2023-04-18T19:28:41.888890Z","shell.execute_reply.started":"2023-04-18T19:28:41.888690Z","shell.execute_reply":"2023-04-18T19:28:41.888713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPOCH = 3\nLR = 0.001\ncriterion = nn.BCELoss()\noptimizerD = optim.Adam(netD.parameters(), lr=LR, betas=(0.5, 0.999))\noptimizerG = optim.Adam(netG.parameters(), lr=LR, betas=(0.5, 0.999))","metadata":{"execution":{"iopub.status.busy":"2023-04-18T19:28:41.890894Z","iopub.status.idle":"2023-04-18T19:28:41.891338Z","shell.execute_reply.started":"2023-04-18T19:28:41.891132Z","shell.execute_reply":"2023-04-18T19:28:41.891155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(EPOCH):\n    for i, data in enumerate(train_loader, 0):\n        print(data)\n        # 1st Step: Updating the weights of the neural network of the discriminator\n        netD.zero_grad()\n        \n        # Training the discriminator with a real image of the dataset\n        real, _, overlay = data\n        input = Variable(real)\n        input = (input.permute(0, 3, 2, 1)).float()\n        target = netD(Variable(torch.ones(input.size())))\n        output = netD(input)\n        print(f'--------------------------\\n{output} : {target}\\n\\n')\n#         target = target.view(output.size()) \n        errD_real = criterion(output, target)\n        \n        # Training the discriminator with a fake image generated by the generator\n#         noise = Variable(torch.randn(input.size()[0], 100, 1, 1))\n        overlay = (overlay.permute(0, 3, 2, 1)).float()\n        fake = netG(overlay)\n        target =  netD(Variable(torch.zeros(input.size())))\n        output = netD(fake.detach())\n        errD_fake = criterion(output, target)\n        \n        # Backpropagating the total error\n        errD = errD_real + errD_fake\n        errD.backward()\n        optimizerD.step()\n        \n        # 2nd Step: Updating the weights of the neural network of the generator\n        netG.zero_grad()\n        target = netD(Variable(torch.ones(input.size())))\n        output = netD(fake)\n        errG = criterion(output, target)\n        errG.backward()\n        optimizerG.step()\n        \n        # 3rd Step: Printing the losses and saving the real images and the generated images of the minibatch every 100 steps\n        print('[%d/%d][%d/%d] Loss_D: %.4f; Loss_G: %.4f' % (epoch, EPOCH, i, len(dataloader), errD.item(), errG.item()))\n        if i % 100 == 0:\n            vutils.save_image(real, '%s/real_samples.png' % \"./results\", normalize=True)\n            fake = netG(overlay)\n            vutils.save_image(fake.data, '%s/fake_samples_epoch_%03d.png' % (\"./results\", epoch), normalize=True)","metadata":{"execution":{"iopub.status.busy":"2023-04-18T19:28:41.893038Z","iopub.status.idle":"2023-04-18T19:28:41.893433Z","shell.execute_reply.started":"2023-04-18T19:28:41.893235Z","shell.execute_reply":"2023-04-18T19:28:41.893256Z"},"trusted":true},"execution_count":null,"outputs":[]}]}